{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 256, 32)           3200      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 28,162\n",
      "Trainable params: 28,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import namedtuple\n",
    "from keract import get_activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "\n",
    "import utils\n",
    "\n",
    "cfg = {}\n",
    "cfg['SGD_BATCHSIZE']    = 64\n",
    "cfg['NUM_EPOCHS']       = 1000\n",
    "cfg['FULL_MI']          = True\n",
    "cfg['ACTIVATION'] = 'relu'\n",
    "cfg['SAVE_DIR'] = 'rawdata/lstm_model'\n",
    "\n",
    "ARCH_NAME =  'lstm'\n",
    "\n",
    "nb_classes = 2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(10)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 100\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 256\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "Y_train = keras.utils.np_utils.to_categorical(y_train, nb_classes).astype('float32')\n",
    "Y_test  = keras.utils.np_utils.to_categorical(y_test, nb_classes).astype('float32')\n",
    "\n",
    "Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "trn = Dataset(X_train, Y_train, y_train, nb_classes)\n",
    "tst = Dataset(X_test , Y_test, y_test, nb_classes)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(64, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "if not os.path.exists(cfg['SAVE_DIR']):\n",
    "            print(\"Making directory\", cfg['SAVE_DIR'])\n",
    "            os.makedirs(cfg['SAVE_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 276s 14ms/step - loss: 0.6360 - acc: 0.6337 - val_loss: 0.6441 - val_acc: 0.6192\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 279s 14ms/step - loss: 0.5923 - acc: 0.6924 - val_loss: 0.5775 - val_acc: 0.7068\n"
     ]
    }
   ],
   "source": [
    "checkpoint = [keras.callbacks.ModelCheckpoint(filepath='rawdata/lstm_model/model_{epoch:04d}.hdf5', save_weights_only=False)]\n",
    "r = model.fit(X_train, Y_train, \n",
    "              batch_size = cfg['SGD_BATCHSIZE'],\n",
    "              epochs = cfg['NUM_EPOCHS'],\n",
    "              validation_split = 0.2,\n",
    "              shuffle = True,\n",
    "              # validation_data=(tst.X, tst.Y),\n",
    "              callbacks = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = cfg['NUM_EPOCHS']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# summarize history for accuracy\n",
    "axes[0].plot(range(1,NUM_EPOCHS+1), r.history['acc'], '-x')\n",
    "axes[0].plot(range(1,NUM_EPOCHS+1), r.history['val_acc'], '-+')\n",
    "axes[0].set_title('Model accuracy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_xticks(range(1,NUM_EPOCHS+1))\n",
    "axes[0].grid(linestyle='-')\n",
    "axes[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "axes[1].plot(range(1,NUM_EPOCHS+1), r.history['loss'], '-x')\n",
    "axes[1].plot(range(1,NUM_EPOCHS+1), r.history['val_loss'], '-+')\n",
    "axes[1].set_title('Model loss')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_xticks(range(1,NUM_EPOCHS+1))\n",
    "axes[1].grid(linestyle='-')\n",
    "axes[1].legend(['train', 'validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rawdata/relu_lstm/epoch00000000\n",
      "Saving rawdata/relu_lstm/epoch00000001\n"
     ]
    }
   ],
   "source": [
    "cur_dir = cfg['SAVE_DIR']\n",
    "i = 0\n",
    "for epochfile in sorted(os.listdir(cur_dir)):\n",
    "    fname = cur_dir + \"/\" + epochfile\n",
    "    model = keras.models.load_model(fname)\n",
    "    \n",
    "    data = {\n",
    "            'activity_tst' : []    # Activity in each layer for test set\n",
    "            }\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'embedding_1':\n",
    "            continue\n",
    "        \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input, outputs=model.get_layer(layer.name).output)\n",
    "        intermediate_output = intermediate_layer_model.predict(X_train)  \n",
    "            \n",
    "        data['activity_tst'].append(intermediate_output)\n",
    "        \n",
    "        del intermediate_layer_model\n",
    "        \n",
    "    fname = 'rawdata/relu_lstm' + \"/epoch%08d\"% i\n",
    "    print(\"Saving\", fname)\n",
    "    with open(fname, 'wb') as f:\n",
    "        cPickle.dump({'ACTIVATION':cfg['ACTIVATION'], 'epoch':i, 'data':data}, f)   \n",
    "    i += 1\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
