{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import loggingreporter \n",
    "\n",
    "cfg = {}\n",
    "cfg['SGD_BATCHSIZE']    = 128\n",
    "cfg['SGD_LEARNINGRATE'] = 0.001\n",
    "cfg['NUM_EPOCHS']       = 5000\n",
    "\n",
    "cfg['ACTIVATION'] = 'relu'\n",
    "# cfg['ACTIVATION'] = 'tanh'\n",
    "# How many hidden neurons to put into each of the layers\n",
    "\n",
    "# Try CNN? BUT by looking at file loggingreporter.py, line 32,33, this piece of code seems only works for dense layer\n",
    "# Why 20, not 2^n, e.g., 16, 32\n",
    "\n",
    "# cfg['LAYER_DIMS'] = [1024, 20, 20, 20]\n",
    "#cfg['LAYER_DIMS'] = [32, 28, 24, 20, 16, 12, 8, 8]\n",
    "cfg['LAYER_DIMS'] = [64, 32, 32, 16, 16] # 0.967 w. 128\n",
    "#cfg['LAYER_DIMS'] = [20, 20, 20, 20, 20, 20] # 0.967 w. 128\n",
    "ARCH_NAME =  '-'.join(map(str,cfg['LAYER_DIMS']))\n",
    "trn, tst = utils.get_mnist()\n",
    "\n",
    "# Seems the code does not support CNN, in the MINST experiment, they unrolled the 28x28 image to 1x784 format...\n",
    "# so dense network can be directly applied to MINST data. Haven't considerred RNN. But if we were to  implement it, \n",
    "# we need to rewrite the functions in loggingreporter.py, not sure we have the time to do it..\n",
    "# Nonetheless, I assume Pokeman is a good alternative.\n",
    "# If we really want to test CNN, then the Gaussian kernel used to estimate MI becomes 2D, kde.py file is easy to modify, but not loggingreporter.py\n",
    "\n",
    "# Where to save activation and weights data\n",
    "cfg['SAVE_DIR'] = 'rawdata/' + cfg['ACTIVATION'] + '_' + ARCH_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 825,390\n",
      "Trainable params: 825,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We should keep the initialization and optimizer the same across different experiments to remove other potential influences\n",
    "# IBnet uses truncatedNormal and Adam\n",
    "\n",
    "input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "\n",
    "clayer = input_layer\n",
    "\n",
    "# Try L2norm, dropout here\n",
    "for n in cfg['LAYER_DIMS']:\n",
    "    clayer = keras.layers.Dense(n, \n",
    "                                activation=cfg['ACTIVATION'],\n",
    "                                kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=1/np.sqrt(float(n)), seed=10),\n",
    "                                bias_initializer='zeros'\n",
    "#                                 kernel_regularizer=regularizers.l2(0.01) # add L2 norm regularization\n",
    "                               )(clayer)\n",
    "#     clayer = keras.layers.Dropout(0.1)(clayer)\n",
    "output_layer = keras.layers.Dense(trn.nb_classes, activation='softmax')(clayer)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "# optimizer = keras.optimizers.SGD(lr=cfg['SGD_LEARNINGRATE'])\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=cfg['SGD_LEARNINGRATE'], momentum=0.9, nesterov=True, clipnorm=1.0, clipvalue=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_report(epoch):\n",
    "    # Only log activity for some epochs.  Mainly this is to make things run faster.\n",
    "    if epoch < 20:       # Log for all first 20 epochs\n",
    "        return True\n",
    "    elif epoch < 100:    # Then for every 5th epoch\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 200:    # Then every 10th\n",
    "        return (epoch % 10 == 0)\n",
    "    else:                # Then every 100th\n",
    "        return (epoch % 100 == 0)\n",
    "    \n",
    "reporter = loggingreporter.LoggingReporter(cfg=cfg, \n",
    "                                          trn=trn, \n",
    "                                          tst=tst, \n",
    "                                          do_save_func=do_report)\n",
    "r = model.fit(x=trn.X, y=trn.Y, \n",
    "              verbose    = 2, \n",
    "              batch_size = cfg['SGD_BATCHSIZE'],\n",
    "              epochs     = cfg['NUM_EPOCHS'],\n",
    "              validation_split = 0.2,\n",
    "              shuffle = True,\n",
    "              # validation_data=(tst.X, tst.Y),\n",
    "              callbacks  = [reporter,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc(\"savefig\", dpi=300)\n",
    "%matplotlib inline\n",
    "\n",
    "NUM_EPOCHS = cfg['NUM_EPOCHS']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# summarize history for accuracy\n",
    "axes[0].plot(range(1,NUM_EPOCHS+1), r.history['acc'], '-x')\n",
    "axes[0].plot(range(1,NUM_EPOCHS+1), r.history['val_acc'], '-+')\n",
    "axes[0].set_title('Model accuracy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_xticks(range(1,NUM_EPOCHS+1))\n",
    "axes[0].grid(linestyle='-')\n",
    "axes[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "axes[1].plot(range(1,NUM_EPOCHS+1), r.history['loss'], '-x')\n",
    "axes[1].plot(range(1,NUM_EPOCHS+1), r.history['val_loss'], '-+')\n",
    "axes[1].set_title('Model loss')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_xticks(range(1,NUM_EPOCHS+1))\n",
    "axes[1].grid(linestyle='-')\n",
    "axes[1].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.savefig('MNIST_acc_loss_plot', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
